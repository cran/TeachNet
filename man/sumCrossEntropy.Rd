\name{sumCrossEntropy}
\alias{sumCrossEntropy}

\title{
Sums up cross entropy
}
\description{
Computes the full value of the cross entropy for TeachNet
}
\usage{
sumCrossEntropy(weights, data, h2)
}

\arguments{
  \item{weights}{
current weights
}
  \item{data}{
data frame
}
  \item{h2}{
number of neurons in second hidden layer
}
}

\author{
Georg Steinbuss
}
\seealso{
\link{squaredError}
}

\keyword{ ~kwd1 }
\keyword{ ~kwd2 }% __ONLY ONE__ keyword per line
